From: Michael Gilbert <mgilbert@debian.org>
Subject: don't set sse2 compiler flags on i386
X-Debian-Bug: http://bugs.debian.org/750361

--- a/build/common.gypi
+++ b/build/common.gypi
@@ -3831,8 +3831,6 @@
                 # http://crbug.com/313032 for an example where this has "bit"
                 # us in the past.
                 'cflags': [
-                  '-msse2',
-                  '-mfpmath=sse',
                   '-mmmx',  # Allows mmintrin.h for MMX intrinsics.
                   '-m32',
                 ],
--- a/cc/cc.gyp
+++ b/cc/cc.gyp
@@ -601,7 +601,7 @@
         '../build/android/increase_size_for_speed.gypi',
       ],
       'conditions': [
-        ['target_arch == "ia32" or target_arch == "x64"', {
+        ['target_arch == "x64"', {
           'sources': [
             'raster/texture_compressor_etc1_sse.cc',
             'raster/texture_compressor_etc1_sse.h',
--- a/cc/raster/texture_compressor.cc
+++ b/cc/raster/texture_compressor.cc
@@ -8,7 +8,7 @@
 #include "base/memory/ptr_util.h"
 #include "cc/raster/texture_compressor_etc1.h"
 
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(ARCH_CPU_X86_64)
 #include "base/cpu.h"
 #include "cc/raster/texture_compressor_etc1_sse.h"
 #endif
@@ -18,7 +18,7 @@ namespace cc {
 std::unique_ptr<TextureCompressor> TextureCompressor::Create(Format format) {
   switch (format) {
     case kFormatETC1: {
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(ARCH_CPU_X86_64)
       base::CPU cpu;
       if (cpu.has_sse2()) {
         return base::WrapUnique(new TextureCompressorETC1SSE());
--- a/media/base/sinc_resampler.cc
+++ b/media/base/sinc_resampler.cc
@@ -84,7 +84,7 @@
 #include "base/logging.h"
 #include "build/build_config.h"
 
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
 #include <xmmintrin.h>
 #define CONVOLVE_FUNC Convolve_SSE
 #elif defined(ARCH_CPU_ARM_FAMILY) && defined(USE_NEON)
@@ -343,7 +343,7 @@ float SincResampler::Convolve_C(const fl
       kernel_interpolation_factor * sum2);
 }
 
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
 float SincResampler::Convolve_SSE(const float* input_ptr, const float* k1,
                                   const float* k2,
                                   double kernel_interpolation_factor) {
--- a/media/base/sinc_resampler.h
+++ b/media/base/sinc_resampler.h
@@ -96,7 +96,7 @@ class MEDIA_EXPORT SincResampler {
   // ARM, NEON support is chosen at compile time based on compilation flags.
   static float Convolve_C(const float* input_ptr, const float* k1,
                           const float* k2, double kernel_interpolation_factor);
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
   static float Convolve_SSE(const float* input_ptr, const float* k1,
                             const float* k2,
                             double kernel_interpolation_factor);
--- a/media/base/sinc_resampler_perftest.cc
+++ b/media/base/sinc_resampler_perftest.cc
@@ -22,7 +22,7 @@ static const double kKernelInterpolation
 static void DoNothing(int frames, float* destination) {}
 
 // Define platform independent function name for Convolve* tests.
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
 #define CONVOLVE_FUNC Convolve_SSE
 #elif defined(ARCH_CPU_ARM_FAMILY) && defined(USE_NEON)
 #define CONVOLVE_FUNC Convolve_NEON
--- a/media/base/sinc_resampler_unittest.cc
+++ b/media/base/sinc_resampler_unittest.cc
@@ -153,7 +153,7 @@ TEST(SincResamplerTest, DISABLED_SetRati
 
 
 // Define platform independent function name for Convolve* tests.
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
 #define CONVOLVE_FUNC Convolve_SSE
 #elif defined(ARCH_CPU_ARM_FAMILY) && defined(USE_NEON)
 #define CONVOLVE_FUNC Convolve_NEON
--- a/media/base/vector_math.cc
+++ b/media/base/vector_math.cc
@@ -11,7 +11,7 @@
 #include "build/build_config.h"
 
 // NaCl does not allow intrinsics.
-#if defined(ARCH_CPU_X86_FAMILY) && !defined(OS_NACL)
+#if defined(__x86_64__) && !defined(OS_NACL)
 #include <xmmintrin.h>
 // Don't use custom SSE versions where the auto-vectorized C version performs
 // better, which is anywhere clang is used.
@@ -89,7 +89,7 @@ std::pair<float, float> EWMAAndMaxPower_
   return result;
 }
 
-#if defined(ARCH_CPU_X86_FAMILY) && !defined(OS_NACL)
+#if defined(__x86_64__) && !defined(OS_NACL)
 void FMUL_SSE(const float src[], float scale, int len, float dest[]) {
   const int rem = len % 4;
   const int last_index = len - rem;
--- a/media/base/vector_math_perftest.cc
+++ b/media/base/vector_math_perftest.cc
@@ -83,7 +83,7 @@ class VectorMathPerfTest : public testin
 };
 
 // Define platform dependent function names for SIMD optimized methods.
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
 #define FMAC_FUNC FMAC_SSE
 #define FMUL_FUNC FMUL_SSE
 #define EWMAAndMaxPower_FUNC EWMAAndMaxPower_SSE
--- a/media/base/vector_math_testing.h
+++ b/media/base/vector_math_testing.h
@@ -19,7 +19,7 @@ MEDIA_EXPORT void FMUL_C(const float src
 MEDIA_EXPORT std::pair<float, float> EWMAAndMaxPower_C(
     float initial_value, const float src[], int len, float smoothing_factor);
 
-#if defined(ARCH_CPU_X86_FAMILY) && !defined(OS_NACL)
+#if defined(__x86_64__) && !defined(OS_NACL)
 MEDIA_EXPORT void FMAC_SSE(const float src[], float scale, int len,
                            float dest[]);
 MEDIA_EXPORT void FMUL_SSE(const float src[], float scale, int len,
--- a/media/base/vector_math_unittest.cc
+++ b/media/base/vector_math_unittest.cc
@@ -76,7 +76,7 @@ TEST_F(VectorMathTest, FMAC) {
     VerifyOutput(kResult);
   }
 
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
   {
     SCOPED_TRACE("FMAC_SSE");
     FillTestVectors(kInputFillValue, kOutputFillValue);
@@ -117,7 +117,7 @@ TEST_F(VectorMathTest, FMUL) {
     VerifyOutput(kResult);
   }
 
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
   {
     SCOPED_TRACE("FMUL_SSE");
     FillTestVectors(kInputFillValue, kOutputFillValue);
@@ -225,7 +225,7 @@ class EWMATestScenario {
       EXPECT_NEAR(expected_max_, result.second, 0.0000001f);
     }
 
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
     {
       SCOPED_TRACE("EWMAAndMaxPower_SSE");
       const std::pair<float, float>& result = vector_math::EWMAAndMaxPower_SSE(
--- a/media/base/yuv_convert.cc
+++ b/media/base/yuv_convert.cc
@@ -32,7 +32,7 @@
 #include "media/base/simd/convert_yuv_to_rgb.h"
 #include "media/base/simd/filter_yuv.h"
 
-#if defined(ARCH_CPU_X86_FAMILY)
+#if defined(__x86_64__)
 #if defined(COMPILER_MSVC)
 #include <intrin.h>
 #else
@@ -245,7 +245,7 @@ void InitializeCPUSpecificYUVConversions
   g_empty_register_state_proc_ = EmptyRegisterStateStub;
 
   // Assembly code confuses MemorySanitizer. Also not available in iOS builds.
-#if defined(ARCH_CPU_X86_FAMILY) && !defined(MEMORY_SANITIZER) && \
+#if defined(ARCH_CPU_X86_64) && !defined(MEMORY_SANITIZER) && \
     !defined(OS_IOS)
   g_convert_yuva_to_argb_proc_ = ConvertYUVAToARGB_MMX;
 
--- a/media/base/yuv_convert.h
+++ b/media/base/yuv_convert.h
@@ -16,7 +16,7 @@
 // hide the versions implemented with heavy use of MMX intrinsics.
 // TODO(wolenetz): Use MMX intrinsics when compiling win64 with Visual
 // Studio 2012? http://crbug.com/173450
-#if defined(ARCH_CPU_X86_FAMILY) && \
+#if defined(__x86_64__) && \
     !(defined(ARCH_CPU_X86_64) && defined(COMPILER_MSVC))
 #define MEDIA_MMX_INTRINSICS_AVAILABLE
 #endif
--- a/media/base/yuv_convert_unittest.cc
+++ b/media/base/yuv_convert_unittest.cc
@@ -871,6 +871,6 @@ TEST(YUVConvertTest, LinearScaleYUVToRGB
 
 #endif  // defined(ARCH_CPU_X86_64)
 
-#endif  // defined(ARCH_CPU_X86_FAMILY)
+#endif  // defined(__x86_64__)
 
 }  // namespace media
--- a/media/media.gyp
+++ b/media/media.gyp
@@ -1016,7 +1016,7 @@
             'filters/ffmpeg_h265_to_annex_b_bitstream_converter.h',
           ],
         }],
-        ['target_arch=="ia32" or target_arch=="x64"', {
+        ['target_arch=="x64"', {
           'dependencies': [
             'media_asm',
           ],
--- a/skia/ext/convolver.h
+++ b/skia/ext/convolver.h
@@ -17,7 +17,6 @@
 // We can build SSE2 optimized versions for all x86 CPUs
 // except when building for the IOS emulator.
 #if defined(ARCH_CPU_X86_FAMILY) && !defined(OS_IOS)
-#define SIMD_SSE2 1
 #define SIMD_PADDING 8  // 8 * int16_t
 #endif
 
--- a/skia/skia_chrome.gypi
+++ b/skia/skia_chrome.gypi
@@ -75,7 +75,7 @@
     [ 'OS != "ios" and target_arch != "arm" and target_arch != "mipsel" and \
        target_arch != "arm64" and target_arch != "mips64el"', {
       'sources': [
-        'ext/convolver_SSE2.cc',
+        'ext/convolver.cc',
       ],
     }],
     [ 'target_arch == "mipsel" and mips_dsp_rev >= 2',{
--- a/skia/skia_library_opts.gyp
+++ b/skia/skia_library_opts.gyp
@@ -51,7 +51,7 @@
           # be in the regular skia target. But we need skia_opts for arm
           # anyway, so putting sse2_sources here is simpler than making this
           # conditionally a type none target on x86.
-          'sources': [ '<@(sse2_sources)' ],
+          'sources': [ '<@(none_sources)' ],
           'dependencies': [
             'skia_opts_ssse3',
             'skia_opts_sse41',
--- a/third_party/WebKit/Source/modules/webaudio/AudioParamTimeline.cpp
+++ b/third_party/WebKit/Source/modules/webaudio/AudioParamTimeline.cpp
@@ -32,7 +32,7 @@
 #include "wtf/MathExtras.h"
 #include <algorithm>
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
 #include <emmintrin.h>
 #endif
 
@@ -575,7 +575,7 @@ float AudioParamTimeline::valuesForFrame
         // First handle linear and exponential ramps which require looking ahead to the next event.
         if (nextEventType == ParamEvent::LinearRampToValue) {
             const float valueDelta = value2 - value1;
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
             // Minimize in-loop operations. Calculate starting value and increment. Next step: value += inc.
             //  value = value1 + (currentFrame/sampleRate - time1) * k * (value2 - value1);
             //  inc = 4 / sampleRate * k * (value2 - value1);
@@ -738,7 +738,7 @@ float AudioParamTimeline::valuesForFrame
                         for (; writeIndex < fillToFrame; ++writeIndex)
                             values[writeIndex] = target;
                     } else {
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
                         // Resolve recursion by expanding constants to achieve a 4-step loop unrolling.
                         // v1 = v0 + (t - v0) * c
                         // v2 = v1 + (t - v1) * c
@@ -843,7 +843,7 @@ float AudioParamTimeline::valuesForFrame
                     // Render the stretched curve data using linear interpolation.  Oversampled
                     // curve data can be provided if sharp discontinuities are desired.
                     unsigned k = 0;
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
                     const __m128 vCurveVirtualIndex = _mm_set_ps1(curveVirtualIndex);
                     const __m128 vCurvePointsPerFrame = _mm_set_ps1(curvePointsPerFrame);
                     const __m128 vNumberOfCurvePointsM1 = _mm_set_ps1(numberOfCurvePoints - 1);
--- a/third_party/WebKit/Source/platform/audio/DirectConvolver.cpp
+++ b/third_party/WebKit/Source/platform/audio/DirectConvolver.cpp
@@ -35,7 +35,7 @@
 #include "platform/audio/VectorMath.h"
 #include "wtf/CPU.h"
 
-#if (CPU(X86) || CPU(X86_64)) && !OS(MACOSX)
+#if (CPU(X86_64)) && !OS(MACOSX)
 #include <emmintrin.h>
 #endif
 
@@ -82,7 +82,7 @@ void DirectConvolver::process(AudioFloat
 #endif // CPU(X86)
 #else
     size_t i = 0;
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     // Convolution using SSE2. Currently only do this if both |kernelSize| and |framesToProcess|
     // are multiples of 4. If not, use the straightforward loop below.
 
@@ -392,7 +392,7 @@ void DirectConvolver::process(AudioFloat
         }
         destP[i++] = sum;
     }
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     }
 #endif
 #endif // OS(MACOSX)
--- a/third_party/WebKit/Source/platform/audio/SincResampler.cpp
+++ b/third_party/WebKit/Source/platform/audio/SincResampler.cpp
@@ -31,7 +31,7 @@
 #include "wtf/CPU.h"
 #include "wtf/MathExtras.h"
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
 #include <emmintrin.h>
 #endif
 
@@ -256,7 +256,7 @@ void SincResampler::process(AudioSourceP
             {
                 float input;
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
                 // If the sourceP address is not 16-byte aligned, the first several frames (at most three) should be processed seperately.
                 while ((reinterpret_cast<uintptr_t>(inputP) & 0x0F) && n) {
                     CONVOLVE_ONE_SAMPLE
--- a/third_party/WebKit/Source/platform/audio/VectorMath.cpp
+++ b/third_party/WebKit/Source/platform/audio/VectorMath.cpp
@@ -32,7 +32,7 @@
 #include <Accelerate/Accelerate.h>
 #endif
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
 #include <emmintrin.h>
 #endif
 
@@ -122,7 +122,7 @@ void vsma(const float* sourceP, int sour
 {
     int n = framesToProcess;
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     if ((sourceStride == 1) && (destStride == 1)) {
         float k = *scale;
 
@@ -195,7 +195,7 @@ void vsmul(const float* sourceP, int sou
 {
     int n = framesToProcess;
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     if ((sourceStride == 1) && (destStride == 1)) {
         float k = *scale;
 
@@ -266,7 +266,7 @@ void vsmul(const float* sourceP, int sou
         sourceP += sourceStride;
         destP += destStride;
     }
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     }
 #endif
 }
@@ -275,7 +275,7 @@ void vadd(const float* source1P, int sou
 {
     int n = framesToProcess;
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     if ((sourceStride1 ==1) && (sourceStride2 == 1) && (destStride == 1)) {
         // If the sourceP address is not 16-byte aligned, the first several frames (at most three) should be processed separately.
         while ((reinterpret_cast<size_t>(source1P) & 0x0F) && n) {
@@ -378,7 +378,7 @@ void vadd(const float* source1P, int sou
         source2P += sourceStride2;
         destP += destStride;
     }
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     }
 #endif
 }
@@ -388,7 +388,7 @@ void vmul(const float* source1P, int sou
 
     int n = framesToProcess;
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     if ((sourceStride1 == 1) && (sourceStride2 == 1) && (destStride == 1)) {
         // If the source1P address is not 16-byte aligned, the first several frames (at most three) should be processed separately.
         while ((reinterpret_cast<uintptr_t>(source1P) & 0x0F) && n) {
@@ -461,7 +461,7 @@ void vmul(const float* source1P, int sou
 void zvmul(const float* real1P, const float* imag1P, const float* real2P, const float* imag2P, float* realDestP, float* imagDestP, size_t framesToProcess)
 {
     unsigned i = 0;
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     // Only use the SSE optimization in the very common case that all addresses are 16-byte aligned.
     // Otherwise, fall through to the scalar code below.
     if (!(reinterpret_cast<uintptr_t>(real1P) & 0x0F)
@@ -519,7 +519,7 @@ void vsvesq(const float* sourceP, int so
     int n = framesToProcess;
     float sum = 0;
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     if (sourceStride == 1) {
         // If the sourceP address is not 16-byte aligned, the first several frames (at most three) should be processed separately.
         while ((reinterpret_cast<uintptr_t>(sourceP) & 0x0F) && n) {
@@ -584,7 +584,7 @@ void vmaxmgv(const float* sourceP, int s
     int n = framesToProcess;
     float max = 0;
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     if (sourceStride == 1) {
         // If the sourceP address is not 16-byte aligned, the first several frames (at most three) should be processed separately.
         while ((reinterpret_cast<uintptr_t>(sourceP) & 0x0F) && n) {
--- a/third_party/WebKit/Source/platform/graphics/cpu/x86/WebGLImageConversionSSE.h
+++ b/third_party/WebKit/Source/platform/graphics/cpu/x86/WebGLImageConversionSSE.h
@@ -5,7 +5,7 @@
 #ifndef WebGLImageConversionSSE_h
 #define WebGLImageConversionSSE_h
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
 
 #include <emmintrin.h>
 
--- a/third_party/WebKit/Source/platform/graphics/gpu/WebGLImageConversion.cpp
+++ b/third_party/WebKit/Source/platform/graphics/gpu/WebGLImageConversion.cpp
@@ -423,7 +423,7 @@ template<> void unpack<WebGLImageConvers
     const uint32_t* source32 = reinterpret_cast_ptr<const uint32_t*>(source);
     uint32_t* destination32 = reinterpret_cast_ptr<uint32_t*>(destination);
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     SIMD::unpackOneRowOfBGRA8LittleToRGBA8(source32, destination32, pixelsPerRow);
 #endif
     for (unsigned i = 0; i < pixelsPerRow; ++i) {
@@ -442,7 +442,7 @@ template<> void unpack<WebGLImageConvers
 
 template<> void unpack<WebGLImageConversion::DataFormatRGBA5551, uint16_t, uint8_t>(const uint16_t* source, uint8_t* destination, unsigned pixelsPerRow)
 {
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     SIMD::unpackOneRowOfRGBA5551LittleToRGBA8(source, destination, pixelsPerRow);
 #endif
 #if HAVE(ARM_NEON_INTRINSICS)
@@ -464,7 +464,7 @@ template<> void unpack<WebGLImageConvers
 
 template<> void unpack<WebGLImageConversion::DataFormatRGBA4444, uint16_t, uint8_t>(const uint16_t* source, uint8_t* destination, unsigned pixelsPerRow)
 {
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     SIMD::unpackOneRowOfRGBA4444LittleToRGBA8(source, destination, pixelsPerRow);
 #endif
 #if HAVE(ARM_NEON_INTRINSICS)
@@ -630,7 +630,7 @@ template<> void pack<WebGLImageConversio
 // FIXME: this routine is lossy and must be removed.
 template<> void pack<WebGLImageConversion::DataFormatR8, WebGLImageConversion::AlphaDoUnmultiply, uint8_t, uint8_t>(const uint8_t* source, uint8_t* destination, unsigned pixelsPerRow)
 {
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     SIMD::packOneRowOfRGBA8LittleToR8(source, destination, pixelsPerRow);
 #endif
     for (unsigned i = 0; i < pixelsPerRow; ++i) {
@@ -667,7 +667,7 @@ template<> void pack<WebGLImageConversio
 // FIXME: this routine is lossy and must be removed.
 template<> void pack<WebGLImageConversion::DataFormatRA8, WebGLImageConversion::AlphaDoUnmultiply, uint8_t, uint8_t>(const uint8_t* source, uint8_t* destination, unsigned pixelsPerRow)
 {
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     SIMD::packOneRowOfRGBA8LittleToRA8(source, destination, pixelsPerRow);
 #endif
     for (unsigned i = 0; i < pixelsPerRow; ++i) {
@@ -741,7 +741,7 @@ template<> void pack<WebGLImageConversio
 // FIXME: this routine is lossy and must be removed.
 template<> void pack<WebGLImageConversion::DataFormatRGBA8, WebGLImageConversion::AlphaDoUnmultiply, uint8_t, uint8_t>(const uint8_t* source, uint8_t* destination, unsigned pixelsPerRow)
 {
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     SIMD::packOneRowOfRGBA8LittleToRGBA8(source, destination, pixelsPerRow);
 #else
     for (unsigned i = 0; i < pixelsPerRow; ++i) {
--- a/third_party/qcms/qcms.gyp
+++ b/third_party/qcms/qcms.gyp
@@ -50,7 +50,7 @@
             'src/transform_util.h',
           ],
           'conditions': [
-            ['target_arch=="ia32" or target_arch=="x64"', {
+            ['target_arch=="x64"', {
               'defines': [
                 'SSE2_ENABLE',
               ],
